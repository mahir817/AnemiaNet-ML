{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_PATH = \"../data/raw/BDIR61FL.csv\"\n",
    "\n",
    "df_raw = pd.read_csv(RAW_PATH)  # df_raw = OG copy\n",
    "df = df_raw.copy()              # df = working copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17842, 1827)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17842 entries, 0 to 17841\n",
      "Columns: 1827 entries, caseid to s541y_3\n",
      "dtypes: float64(1599), int64(224), object(4)\n",
      "memory usage: 248.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.head()\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_cols = df.columns[df.isnull().all()]\n",
    "len(null_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns with only ONE unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constant_cols = df.columns[df.nunique() <= 1]\n",
    "len(constant_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=constant_cols, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identity Info drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_like_cols = [\n",
    "    \"caseid\", \"hhid\", \"cluster\", \"line\", \"v001\", \"v002\", \"v003\", \"v004\", \"v005\", \"v006\", \"v007\",\"v008\",\"v009\"\n",
    "]\n",
    "\n",
    "df.drop(columns=id_like_cols, errors=\"ignore\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m CLEAN_PATH = \u001b[33m\"\u001b[39m\u001b[33m../data/processed/01_basic_cleaned.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCLEAN_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mahir\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mahir\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[39m, in \u001b[36mNDFrame.to_csv\u001b[39m\u001b[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[39m\n\u001b[32m   3956\u001b[39m df = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m   3958\u001b[39m formatter = DataFrameFormatter(\n\u001b[32m   3959\u001b[39m     frame=df,\n\u001b[32m   3960\u001b[39m     header=header,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3964\u001b[39m     decimal=decimal,\n\u001b[32m   3965\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3967\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3970\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3972\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3973\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3974\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3975\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3976\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3977\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3980\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3981\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3982\u001b[39m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3983\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3984\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mahir\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[39m, in \u001b[36mDataFrameRenderer.to_csv\u001b[39m\u001b[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[39m\n\u001b[32m    993\u001b[39m     created_buffer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    995\u001b[39m csv_formatter = CSVFormatter(\n\u001b[32m    996\u001b[39m     path_or_buf=path_or_buf,\n\u001b[32m    997\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1012\u001b[39m     formatter=\u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m   1013\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m \u001b[43mcsv_formatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mahir\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:270\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[32m    252\u001b[39m     \u001b[38;5;28mself\u001b[39m.filepath_or_buffer,\n\u001b[32m    253\u001b[39m     \u001b[38;5;28mself\u001b[39m.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    258\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    261\u001b[39m         handles.handle,\n\u001b[32m    262\u001b[39m         lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m         quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    268\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mahir\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:275\u001b[39m, in \u001b[36mCSVFormatter._save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._need_to_save_header:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28mself\u001b[39m._save_header()\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mahir\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:313\u001b[39m, in \u001b[36mCSVFormatter._save_body\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m start_i >= end_i:\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_i\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mahir\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:320\u001b[39m, in \u001b[36mCSVFormatter._save_chunk\u001b[39m\u001b[34m(self, start_i, end_i)\u001b[39m\n\u001b[32m    317\u001b[39m slicer = \u001b[38;5;28mslice\u001b[39m(start_i, end_i)\n\u001b[32m    318\u001b[39m df = \u001b[38;5;28mself\u001b[39m.obj.iloc[slicer]\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m res = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_values_for_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_number_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m data = \u001b[38;5;28mlist\u001b[39m(res._iter_column_arrays())\n\u001b[32m    323\u001b[39m ix = \u001b[38;5;28mself\u001b[39m.data_index[slicer]._get_values_for_csv(**\u001b[38;5;28mself\u001b[39m._number_format)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mahir\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:1410\u001b[39m, in \u001b[36mDataFrame._get_values_for_csv\u001b[39m\u001b[34m(self, float_format, date_format, decimal, na_rep, quoting)\u001b[39m\n\u001b[32m   1400\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_values_for_csv\u001b[39m(\n\u001b[32m   1401\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1402\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1408\u001b[39m ) -> Self:\n\u001b[32m   1409\u001b[39m     \u001b[38;5;66;03m# helper used by to_csv\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1410\u001b[39m     mgr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_values_for_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfloat_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfloat_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_rep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_rep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1415\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1416\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1417\u001b[39m     \u001b[38;5;66;03m# error: Incompatible return value type (got \"DataFrame\", expected \"Self\")\u001b[39;00m\n\u001b[32m   1418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(mgr, axes=mgr.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mahir\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:466\u001b[39m, in \u001b[36mBaseBlockManager.get_values_for_csv\u001b[39m\u001b[34m(self, float_format, date_format, decimal, na_rep, quoting)\u001b[39m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_values_for_csv\u001b[39m(\n\u001b[32m    460\u001b[39m     \u001b[38;5;28mself\u001b[39m, *, float_format, date_format, decimal, na_rep: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mnan\u001b[39m\u001b[33m\"\u001b[39m, quoting=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    461\u001b[39m ) -> Self:\n\u001b[32m    462\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    463\u001b[39m \u001b[33;03m    Convert values to native types (strings / python objects) that are used\u001b[39;00m\n\u001b[32m    464\u001b[39m \u001b[33;03m    in formatting (repr / csv).\u001b[39;00m\n\u001b[32m    465\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget_values_for_csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_rep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_rep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfloat_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfloat_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mahir\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m         applied = b.apply(f, **kwargs)\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         applied = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001b[32m    366\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mahir\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:780\u001b[39m, in \u001b[36mBlock.get_values_for_csv\u001b[39m\u001b[34m(self, float_format, date_format, decimal, na_rep, quoting)\u001b[39m\n\u001b[32m    775\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_values_for_csv\u001b[39m(\n\u001b[32m    777\u001b[39m     \u001b[38;5;28mself\u001b[39m, *, float_format, date_format, decimal, na_rep: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mnan\u001b[39m\u001b[33m\"\u001b[39m, quoting=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    778\u001b[39m ) -> Block:\n\u001b[32m    779\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"convert to our native types format\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m780\u001b[39m     result = \u001b[43mget_values_for_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_rep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_rep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfloat_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfloat_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    788\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.make_block(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mahir\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7838\u001b[39m, in \u001b[36mget_values_for_csv\u001b[39m\u001b[34m(values, date_format, na_rep, quoting, float_format, decimal)\u001b[39m\n\u001b[32m   7835\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   7836\u001b[39m     values = np.array(values, dtype=\u001b[33m\"\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m7838\u001b[39m \u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m = na_rep\n\u001b[32m   7839\u001b[39m values = values.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   7840\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "CLEAN_PATH = \"../data/processed/01_basic_cleaned.csv\"\n",
    "df.to_csv(CLEAN_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: (17842, 1827)\n",
      "After : (17842, 1428)\n"
     ]
    }
   ],
   "source": [
    "print(\"Before:\", df_raw.shape)\n",
    "print(\"After :\", df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHASE 2: Feature Decoding and Interpretation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['v000', 'v010', 'v011', 'v012', 'v013', 'v014', 'v015', 'v016', 'v017',\n",
       "       'v018', 'v019', 'v019a', 'v020', 'v021', 'v022', 'v023', 'v024', 'v025',\n",
       "       'v026', 'v027', 'v028', 'v029', 'v030', 'v031', 'v032', 'v034', 'v040',\n",
       "       'v042', 'v101', 'v102', 'v106', 'v107', 'v113', 'v115', 'v116', 'v119',\n",
       "       'v120', 'v121', 'v122', 'v123', 'v124', 'v127', 'v128', 'v129', 'v130',\n",
       "       'v133', 'v134', 'v135', 'v136', 'v137'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[:50]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search anima variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shb1_1',\n",
       " 'shb1_2',\n",
       " 'shb1_3',\n",
       " 'shb1_4',\n",
       " 'shb1d_1',\n",
       " 'shb1d_2',\n",
       " 'shb1d_3',\n",
       " 'shb1d_4',\n",
       " 'shb1m_1',\n",
       " 'shb1m_2',\n",
       " 'shb1m_3',\n",
       " 'shb1m_4',\n",
       " 'shb1y_1',\n",
       " 'shb1y_2',\n",
       " 'shb1y_3',\n",
       " 'shb1y_4',\n",
       " 'shb2_1',\n",
       " 'shb2_2',\n",
       " 'shb2_3',\n",
       " 'shb2_4',\n",
       " 'shb2d_1',\n",
       " 'shb2d_2',\n",
       " 'shb2d_3',\n",
       " 'shb2d_4',\n",
       " 'shb2m_1',\n",
       " 'shb2m_2',\n",
       " 'shb2m_3',\n",
       " 'shb2m_4',\n",
       " 'shb2y_1',\n",
       " 'shb2y_2',\n",
       " 'shb2y_3',\n",
       " 'shb2y_4',\n",
       " 'shb3_1',\n",
       " 'shb3_2',\n",
       " 'shb3_3',\n",
       " 'shb3_4',\n",
       " 'shb3d_1',\n",
       " 'shb3d_2',\n",
       " 'shb3d_3',\n",
       " 'shb3d_4',\n",
       " 'shb3m_1',\n",
       " 'shb3m_2',\n",
       " 'shb3m_3',\n",
       " 'shb3m_4',\n",
       " 'shb3y_1',\n",
       " 'shb3y_2',\n",
       " 'shb3y_3',\n",
       " 'shb3y_4']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[col for col in df.columns if \"hb\" in col.lower() or \"anem\" in col.lower()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemoglobin Level\tV453 / HA53\t                  \n",
    "Anemia Status\tV457 / HA57\tAnemia level                     \n",
    "we will name 'hb' for level and 'anemia' for binary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hb\"] = df[\"v453\"].replace(999, pd.NA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     5727.0\n",
       "unique     104.0\n",
       "top        123.0\n",
       "freq       191.0\n",
       "Name: hb, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"hb\"].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create binary anemia label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"anemia\"] = (df[\"hb\"] < 12.0).astype(int) #non-pregnant women\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pregnant'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mahir\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'pregnant'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33manemia\u001b[39m\u001b[33m\"\u001b[39m] = np.where(                                \u001b[38;5;66;03m#pregnant women       \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     (\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpregnant\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m == \u001b[32m1\u001b[39m) & (df[\u001b[33m\"\u001b[39m\u001b[33mhb\u001b[39m\u001b[33m\"\u001b[39m] < \u001b[32m11.0\u001b[39m), \u001b[32m1\u001b[39m,\n\u001b[32m      3\u001b[39m     np.where(df[\u001b[33m\"\u001b[39m\u001b[33mhb\u001b[39m\u001b[33m\"\u001b[39m] < \u001b[32m12.0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m)\n\u001b[32m      4\u001b[39m ) \n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mahir\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mahir\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'pregnant'"
     ]
    }
   ],
   "source": [
    "df[\"anemia\"] = np.where(                                #pregnant women       \n",
    "    (df[\"pregnant\"] == 1) & (df[\"hb\"] < 11.0), 1,\n",
    "    np.where(df[\"hb\"] < 12.0, 1, 0)\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare v and h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hb</th>\n",
       "      <th>v457</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10.4</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13.6</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>11.8</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      hb  v457\n",
       "5   11.0   3.0\n",
       "9   13.0   4.0\n",
       "17  10.4   3.0\n",
       "20  13.6   4.0\n",
       "26  11.8   3.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"hb\", \"v457\"]].dropna().head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fix g/dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hb\"] = df[\"hb\"].replace(994, pd.NA) / 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2.6 to 2.8 Feature Selection  and Encoding  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.google.com/spreadsheets/d/1OCGH5YABR8Lu_VlW97hbYaG_agCxWnVgf20BlLgSswU/edit?gid=2052048461#gid=2052048461"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Panic button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DROP COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keep_cols = [\n",
    "    \"hb\",          # target source \n",
    "    # --- A. Demographic Characteristics ---\n",
    "    'v012', # Age\n",
    "    'v501', # Marital Status\n",
    "    'v025', # Residence (Urban/Rural)\n",
    "    'v024', # Region\n",
    "    'v136', # Household Size\n",
    "    'v511', # Age at 1st Marriage\n",
    "    'v218', # Number of Living Children\n",
    "    'v222', # Birth Interval\n",
    "    'v228', # Terminated pregnancy\n",
    "\n",
    "    # --- B. Socio-Economic Status ---\n",
    "    'v106', # Education Level\n",
    "    'v133', # Years of Schooling\n",
    "    'v190', # Wealth Index\n",
    "    'v714', # Employment Status\n",
    "    'v716', # Occupation\n",
    "    'v119', # Electricity\n",
    "    'v127', # Main Floor Material\n",
    "    'v129', # Main Roof Material\n",
    "    'v128', # Main Wall Material\n",
    "    'v121', # Has Television\n",
    "    'v153', # Has Telephone/Mobile (Check if v169a is better)\n",
    "    \n",
    "    # --- C. Lifestyle & Behavioral Factors ---\n",
    "    'v161', # Cooking Fuel\n",
    "    \n",
    "    'v157', # Frequency reading newspaper\n",
    "    'v158', # Frequency listening to radio\n",
    "    'v159', # Frequency watching TV\n",
    "    'v312', # Current Contraceptive Method\n",
    "\n",
    "    # --- D. Environmental & Sanitation ---\n",
    "    'v113', # Source of Drinking Water\n",
    "    'v116', # Type of Toilet Facility\n",
    "    'v160', # Shared Toilet\n",
    "    'v115', # Time to get to Water Source\n",
    "    'v040', # Cluster Altitude (meters)\n",
    "\n",
    "    # --- E. Health & Nutrition Indicators ---\n",
    "    'v445', # Body Mass Index (BMI)\n",
    "    'v213', # Pregnancy Status\n",
    "    'v404', # Currently Breastfeeding\n",
    "    'v215', # Time since last menstrual period\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[keep_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "occupation              15149\n",
       "hb                      12137\n",
       "birth_interval           1817\n",
       "shared_toilet             606\n",
       "age                         0\n",
       "marital_status              0\n",
       "household_size              0\n",
       "age_first_marriage          0\n",
       "living_children             0\n",
       "terminated_pregnancy        0\n",
       "residence                   0\n",
       "education_level             0\n",
       "years_schooling             0\n",
       "wealth_index                0\n",
       "employment_status           0\n",
       "has_electricity             0\n",
       "floor_material              0\n",
       "roof_material               0\n",
       "region                      0\n",
       "wall_material               0\n",
       "has_tv                      0\n",
       "cooking_fuel                0\n",
       "has_telephone               0\n",
       "freq_radio                  0\n",
       "freq_tv                     0\n",
       "contraceptive_method        0\n",
       "freq_newspaper              0\n",
       "water_source                0\n",
       "toilet_type                 0\n",
       "time_to_water               0\n",
       "altitude                    0\n",
       "bmi                         0\n",
       "is_pregnant                 0\n",
       "is_breastfeeding            0\n",
       "time_since_period           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "df.head()\n",
    "df.isnull().sum().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename Col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\n",
    "    # --- A. Demographic Characteristics ---\n",
    "    \"v012\": \"age\",\n",
    "    \"v501\": \"marital_status\",\n",
    "    \"v025\": \"residence\",\n",
    "    \"v024\": \"region\",\n",
    "    \"v136\": \"household_size\",\n",
    "    \"v511\": \"age_first_marriage\",\n",
    "    \"v218\": \"living_children\",\n",
    "    \"v222\": \"birth_interval\",\n",
    "    \"v228\": \"terminated_pregnancy\",\n",
    "\n",
    "    # --- B. Socio-Economic Status ---\n",
    "    \"v106\": \"education_level\",\n",
    "    \"v133\": \"years_schooling\",\n",
    "    \"v190\": \"wealth_index\",\n",
    "    \"v714\": \"employment_status\",\n",
    "    \"v716\": \"occupation\",\n",
    "    \"v119\": \"has_electricity\",\n",
    "    \"v127\": \"floor_material\",\n",
    "    \"v129\": \"roof_material\",\n",
    "    \"v128\": \"wall_material\",\n",
    "    \"v121\": \"has_tv\",\n",
    "    \"v153\": \"has_telephone\",       # Note: v153 is often landline. Check if v169a (mobile) is available/better.\n",
    "\n",
    "    # --- C. Lifestyle & Behavioral Factors ---\n",
    "    \"v161\": \"cooking_fuel\",\n",
    "    \"v157\": \"freq_newspaper\",\n",
    "    \"v158\": \"freq_radio\",\n",
    "    \"v159\": \"freq_tv\",\n",
    "    \"v312\": \"contraceptive_method\",\n",
    "\n",
    "    # --- D. Environmental & Sanitation ---\n",
    "    \"v113\": \"water_source\",\n",
    "    \"v116\": \"toilet_type\",\n",
    "    \"v160\": \"shared_toilet\",\n",
    "    \"v115\": \"time_to_water\",\n",
    "    \"v040\": \"altitude\",\n",
    "\n",
    "    # --- E. Health & Nutrition Indicators ---\n",
    "    \"v445\": \"bmi\",\n",
    "    \"v213\": \"is_pregnant\",\n",
    "    \"v404\": \"is_breastfeeding\",\n",
    "    \"v215\": \"time_since_period\",\n",
    "    \n",
    "    # --- Target Variables (If you haven't renamed them yet) ---\n",
    "    \"v453\": \"hg\",\n",
    "    \n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/processed/feature_reduced.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROC_PATH = \"../data/processed/feature_reduced.csv\"\n",
    "\n",
    "df = pd.read_csv(PROC_PATH)             # df = working copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17842, 35)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Noise or redundancy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    \"iron_supplementation\",\n",
    "    \"birth_interval\",\n",
    "    \"terminated_pregnancy\",\n",
    "    \"time_since_period\",\n",
    "    \"years_schooling\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=cols_to_drop, inplace=True, errors=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17842, 31)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/processed/feature_reduced_2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RollBack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before_missing = df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Missing value per Col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_count</th>\n",
       "      <th>missing_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>occupation</th>\n",
       "      <td>15149</td>\n",
       "      <td>84.906401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hb</th>\n",
       "      <td>12137</td>\n",
       "      <td>68.024885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shared_toilet</th>\n",
       "      <td>606</td>\n",
       "      <td>3.396480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>residence</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>household_size</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_first_marriage</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>living_children</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wealth_index</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education_level</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>employment_status</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_electricity</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>floor_material</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marital_status</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roof_material</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wall_material</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_telephone</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_tv</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq_newspaper</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq_radio</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq_tv</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cooking_fuel</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contraceptive_method</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>water_source</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toilet_type</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_to_water</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>altitude</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bmi</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_pregnant</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_breastfeeding</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      missing_count  missing_percent\n",
       "occupation                    15149        84.906401\n",
       "hb                            12137        68.024885\n",
       "shared_toilet                   606         3.396480\n",
       "residence                         0         0.000000\n",
       "region                            0         0.000000\n",
       "household_size                    0         0.000000\n",
       "age                               0         0.000000\n",
       "age_first_marriage                0         0.000000\n",
       "living_children                   0         0.000000\n",
       "wealth_index                      0         0.000000\n",
       "education_level                   0         0.000000\n",
       "employment_status                 0         0.000000\n",
       "has_electricity                   0         0.000000\n",
       "floor_material                    0         0.000000\n",
       "marital_status                    0         0.000000\n",
       "roof_material                     0         0.000000\n",
       "wall_material                     0         0.000000\n",
       "has_telephone                     0         0.000000\n",
       "has_tv                            0         0.000000\n",
       "freq_newspaper                    0         0.000000\n",
       "freq_radio                        0         0.000000\n",
       "freq_tv                           0         0.000000\n",
       "cooking_fuel                      0         0.000000\n",
       "contraceptive_method              0         0.000000\n",
       "water_source                      0         0.000000\n",
       "toilet_type                       0         0.000000\n",
       "time_to_water                     0         0.000000\n",
       "altitude                          0         0.000000\n",
       "bmi                               0         0.000000\n",
       "is_pregnant                       0         0.000000\n",
       "is_breastfeeding                  0         0.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_counts = df.isna().sum().sort_values(ascending=False)\n",
    "missing_percent = (missing_counts / len(df)) * 100\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    \"missing_count\": missing_counts,\n",
    "    \"missing_percent\": missing_percent\n",
    "})\n",
    "\n",
    "missing_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    \"occupation\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=cols_to_drop, inplace=True, errors=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop rows with missing target  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17842, 31)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"hb\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5705, 31)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE BINARY LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mahir\\AppData\\Local\\Temp\\ipykernel_3864\\3435557492.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"anemia\"] = (df[\"hb\"] < 12.0).astype(int)\n"
     ]
    }
   ],
   "source": [
    "df[\"anemia\"] = (df[\"hb\"] < 12.0).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/processed/feature_reduced_3.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phase 4 STEP 5.1. (start from here tomorrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROC_PATH = \"../data/processed/feature_reduced_3.csv\"\n",
    "\n",
    "df = pd.read_csv(PROC_PATH)             # df = working copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5705, 31)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    \"occupation\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/processed/processed_4.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROC_PATH = \"../data/processed/processed_4.csv\"\n",
    "\n",
    "df = pd.read_csv(PROC_PATH)             # df = working copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5705, 31)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hb                      float64\n",
       "age                       int64\n",
       "marital_status            int64\n",
       "residence                 int64\n",
       "region                    int64\n",
       "household_size            int64\n",
       "age_first_marriage        int64\n",
       "living_children           int64\n",
       "education_level           int64\n",
       "wealth_index              int64\n",
       "employment_status         int64\n",
       "has_electricity           int64\n",
       "floor_material            int64\n",
       "roof_material             int64\n",
       "wall_material             int64\n",
       "has_tv                    int64\n",
       "has_telephone             int64\n",
       "cooking_fuel              int64\n",
       "freq_newspaper            int64\n",
       "freq_radio                int64\n",
       "freq_tv                   int64\n",
       "contraceptive_method      int64\n",
       "water_source              int64\n",
       "toilet_type               int64\n",
       "shared_toilet           float64\n",
       "time_to_water             int64\n",
       "altitude                  int64\n",
       "bmi                       int64\n",
       "is_pregnant               int64\n",
       "is_breastfeeding          int64\n",
       "anemia                    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINAL FEATURE CATEGORIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Numerical Features (Continuous or Counts)\n",
    "# Strategy: Impute missing with Median/Mean -> StandardScaler\n",
    "numeric_features = [\n",
    "    \"age\",                  # v012\n",
    "    \"household_size\",       # v136\n",
    "    \"age_first_marriage\",   # v511\n",
    "    \"living_children\",      # v218\n",
    "    \"years_schooling\",      # v133\n",
    "    \"time_to_water\",        # v115 (Ensure 996 is recoded to 0 first)\n",
    "    \"altitude\",             # v040\n",
    "    \"bmi\"                   # v445\n",
    "]\n",
    "\n",
    "# 2. Ordinal Categorical Features (Rank/Order matters)\n",
    "# Strategy: OrdinalEncoder (Preserve 0<1<2 hierarchy)\n",
    "ordinal_features = [\n",
    "    \"education_level\",      # v106 (No < Pri < Sec < Higher)\n",
    "    \"wealth_index\",         # v190 (Poorest < ... < Richest)\n",
    "    \"freq_newspaper\",       # v157 (Not at all < ... < Often)\n",
    "    \"freq_radio\",           # v158\n",
    "    \"freq_tv\"               # v159\n",
    "]\n",
    "\n",
    "# 3. Nominal Categorical Features (No inherent order)\n",
    "# Strategy: OneHotEncoder (get_dummies)\n",
    "nominal_features = [\n",
    "    \"marital_status\",       # v501 (Married, Widowed, Divorced...)\n",
    "    \"residence\",            # v025 (Urban, Rural) - Technically binary but safe here\n",
    "    \"region\",               # v024\n",
    "    \"floor_material\",       # v127\n",
    "    \"roof_material\",        # v129\n",
    "    \"wall_material\",        # v128\n",
    "    \"cooking_fuel\",         # v161\n",
    "    \"water_source\",         # v113\n",
    "    \"toilet_type\",          # v116\n",
    "    \"contraceptive_method\", # v312\n",
    "]\n",
    "\n",
    "# 4. Binary Features (True/False or 0/1)\n",
    "# Strategy: Simple Mapping (No=0, Yes=1) -> Mode Imputation\n",
    "binary_features = [\n",
    "    \"employment_status\",    # v714\n",
    "    \"has_electricity\",      # v119\n",
    "    \"has_tv\",               # v121\n",
    "    \"has_telephone\",        # v153\n",
    "    \"is_pregnant\",          # v213\n",
    "    \"is_breastfeeding\",     # v404\n",
    "    \"shared_toilet\",        # v160\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check  did we miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anemia'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features = (\n",
    "    numeric_features +\n",
    "    ordinal_features +\n",
    "    nominal_features +\n",
    "    binary_features +\n",
    "    [\"hb\"]  # target\n",
    ")\n",
    "\n",
    "set(df.columns) - set(all_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = [\"anemia\", \"hb\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features = (\n",
    "    numeric_features +\n",
    "    ordinal_features +\n",
    "    nominal_features +\n",
    "    binary_features +\n",
    "    target_columns\n",
    ")\n",
    "\n",
    "set(df.columns) - set(all_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lock feature types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "feature_groups = {\n",
    "    \"numeric\": numeric_features,\n",
    "    \"ordinal\": ordinal_features,\n",
    "    \"nominal\": nominal_features,\n",
    "    \"binary\": binary_features,\n",
    "    \"target\": target_columns\n",
    "}\n",
    "\n",
    "with open(\"feature_groups.json\", \"w\") as f:\n",
    "    json.dump(feature_groups, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DHS missing codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Handle Special DHS Codes FIRST (Valid Data)\n",
    "df['time_to_water'] = df['time_to_water'].replace(996, 0)\n",
    "\n",
    "# v215 (Time since period) - Complex! \n",
    "# 994=Menopause, 995=Before Last Birth, 996=Never. \n",
    "# You might want to keep these as categories or specific numbers, NOT NaNs.\n",
    "\n",
    "# 2. Define \"True Missing\" mappings\n",
    "missing_map = {\n",
    "    # 1-digit variables (Check carefuly: is 8 a category?)\n",
    "    'education_level': [9],\n",
    "    'wealth_index': [], # Usually no missing\n",
    "    \n",
    "    # 2-digit variables\n",
    "    'age_first_marriage': [99, 98],\n",
    "    'years_schooling': [98, 99],\n",
    "    \n",
    "    # 3-digit variables\n",
    "    'hemoglobin_level': [999], # v453\n",
    "    'birth_interval': [999],   # v222\n",
    "    \n",
    "    # 4-digit variables\n",
    "    'bmi': [9998, 9999],       # v445\n",
    "    'altitude': [9999]         # v040\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Apply the mapping\n",
    "for col, missing_codes in missing_map.items():\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace(missing_codes, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "education_level\n",
       "2    2058\n",
       "1    1757\n",
       "0    1462\n",
       "3     428\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify \n",
    "df[\"education_level\"].value_counts(dropna=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employment_status [0 1]\n",
      "has_electricity [0 1 7]\n",
      "has_tv [0 1 7]\n",
      "has_telephone [0 7 1]\n",
      "is_pregnant [0 1]\n",
      "is_breastfeeding [0 1]\n",
      "shared_toilet [0. 7. 1.]\n"
     ]
    }
   ],
   "source": [
    "for col in binary_features:\n",
    "    print(col, df[col].dropna().unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[binary_features] = df[binary_features].replace(7, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employment_status [0 1]\n",
      "has_electricity [0. 1.]\n",
      "has_tv [0. 1.]\n",
      "has_telephone [0. 1.]\n",
      "is_pregnant [0 1]\n",
      "is_breastfeeding [0 1]\n",
      "shared_toilet [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "for col in binary_features:\n",
    "    print(col, df[col].dropna().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lock Binary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[binary_features] = df[binary_features].astype(\"float\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/processed/processed_5.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- education_level ---\n",
      "education_level\n",
      "0    1462\n",
      "1    1757\n",
      "2    2058\n",
      "3     428\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "--- wealth_index ---\n",
      "wealth_index\n",
      "1    1010\n",
      "2    1063\n",
      "3    1085\n",
      "4    1208\n",
      "5    1339\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "--- freq_newspaper ---\n",
      "freq_newspaper\n",
      "0    4722\n",
      "1     575\n",
      "2     404\n",
      "9       4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "--- freq_radio ---\n",
      "freq_radio\n",
      "0    5154\n",
      "1     242\n",
      "2     307\n",
      "9       2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "--- freq_tv ---\n",
      "freq_tv\n",
      "0    2175\n",
      "1     693\n",
      "2    2835\n",
      "9       2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in ordinal_features:\n",
    "    print(f\"--- {col} ---\")\n",
    "    print(df[col].value_counts(dropna=False).sort_index())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"freq_radio\" in df.columns)\n",
    "print(\"freq_tv\" in df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the strict logical order\n",
    "# Any value NOT in these dictionaries will become NaN (automatically fixing your '9' problem)\n",
    "\n",
    "ordinal_mappings = {\n",
    "    \"education_level\": {0: 0, 1: 1, 2: 2, 3: 3},    # Enforces 0-3\n",
    "    \"wealth_index\":    {1: 0, 2: 1, 3: 2, 4: 3, 5: 4}, # Shifts 1-5 -> 0-4\n",
    "    \n",
    "    # Media: 0=None, 1=Less than weekly, 2=Weekly+\n",
    "    # We map 0,1,2. If your data has 3 (Daily), add 3:3. \n",
    "    # The '9' will disappear because it is not in this map.\n",
    "    \"freq_newspaper\": {0: 0, 1: 1, 2: 2, 3: 3}, \n",
    "    \"freq_radio\":     {0: 0, 1: 1, 2: 2, 3: 3},\n",
    "    \"freq_tv\":        {0: 0, 1: 1, 2: 2, 3: 3}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping education_level...\n",
      "Mapping wealth_index...\n",
      "Mapping freq_newspaper...\n",
      "Mapping freq_radio...\n",
      "Mapping freq_tv...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Apply the mapping safely\n",
    "for col, mapping in ordinal_mappings.items():\n",
    "    if col in df.columns:\n",
    "        print(f\"Mapping {col}...\")\n",
    "        df[col] = df[col].map(mapping)\n",
    "    else:\n",
    "        print(f\"Skipping {col} (not in DataFrame)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Checks ---\n",
      "Wealth unique values (should be 0-4): [1 3 2 0 4]\n",
      "Newspaper unique values (should be 0-2/3, no 9): [ 0.  1.  2. nan]\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check\n",
    "print(\"\\n--- Final Checks ---\")\n",
    "print(\"Wealth unique values (should be 0-4):\", df[\"wealth_index\"].unique())\n",
    "print(\"Newspaper unique values (should be 0-2/3, no 9):\", df[\"freq_newspaper\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lock Ordinal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordinal dtypes clean:\n",
      "education_level    float64\n",
      "wealth_index       float64\n",
      "freq_newspaper     float64\n",
      "freq_radio         float64\n",
      "freq_tv            float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df[ordinal_features] = df[ordinal_features].astype(\"float\")\n",
    "# Sanity check\n",
    "print(\"Ordinal dtypes clean:\")\n",
    "print(df[ordinal_features].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/processed/processed_6.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fix BMI (Special DHS Logic)\n",
    "# Replace DHS missing codes for BMI (9998, 9999) with NaN\n",
    "df[\"bmi\"] = df[\"bmi\"].replace([9998, 9999], np.nan)\n",
    "\n",
    "# Convert to correct unit (div by 100) -> 2500 becomes 25.0\n",
    "# Only divide valid values (NaN stays NaN)\n",
    "df[\"bmi\"] = df[\"bmi\"] / 100.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Fix other numeric missing codes\n",
    "# For variables like age_first_marriage, 97+ often means inconsistent/missing\n",
    "# We replace common numeric missing codes carefully\n",
    "numeric_missing_codes = [99, 98, 999, 998]\n",
    "\n",
    "# Apply ONLY to specific columns where these are definitely errors\n",
    "# (Don't apply to 'age' unless you are sure no one is 98!)\n",
    "cols_to_clean = [\"age_first_marriage\", \"altitude\"] # adjust based on your list\n",
    "\n",
    "for col in cols_to_clean:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace(numeric_missing_codes, np.nan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BMI Stats (Should be approx 12-60):\n",
      "count    5685.000000\n",
      "mean       21.502130\n",
      "std         3.907806\n",
      "min        13.070000\n",
      "25%        18.600000\n",
      "50%        20.870000\n",
      "75%        23.910000\n",
      "max        46.470000\n",
      "Name: bmi, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 3. Sanity Check\n",
    "print(\"BMI Stats (Should be approx 12-60):\")\n",
    "print(df[\"bmi\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Fix Time to Water\n",
    "# 996 = On premises (0 minutes)\n",
    "# 998/999 = Missing\n",
    "if \"time_to_water\" in df.columns:\n",
    "    df[\"time_to_water\"] = df[\"time_to_water\"].replace(996, 0)\n",
    "    df[\"time_to_water\"] = df[\"time_to_water\"].replace([998, 999], np.nan)\n",
    "\n",
    "# 2.3 Fix Altitude\n",
    "# 9999 = Missing\n",
    "if \"altitude\" in df.columns:\n",
    "    df[\"altitude\"] = df[\"altitude\"].replace(9999, np.nan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active numeric features: 7\n",
      " years_schooling is gone from the list.\n",
      "Cleaning general numeric columns: ['household_size', 'age_first_marriage', 'living_children']\n",
      "\n",
      " Numeric cleaning finished successfully.\n"
     ]
    }
   ],
   "source": [
    "# 1. Force-update the feature lists by checking what actually exists in df\n",
    "# This guarantees 'years_schooling' cannot be in this list\n",
    "numeric_features = [c for c in numeric_features if c in df.columns]\n",
    "ordinal_features = [c for c in ordinal_features if c in df.columns]\n",
    "nominal_features = [c for c in nominal_features if c in df.columns]\n",
    "\n",
    "print(f\"Active numeric features: {len(numeric_features)}\")\n",
    "if \"years_schooling\" in numeric_features:\n",
    "    print(\" ERROR: years_schooling is STILL in the list.\")\n",
    "else:\n",
    "    print(\" years_schooling is gone from the list.\")\n",
    "\n",
    "# 2. Re-define the 'other' bucket using the CLEAN list\n",
    "# We exclude special columns that need specific logic\n",
    "special_numerics = [\"bmi\", \"time_to_water\", \"altitude\", \"age\"]\n",
    "other_numeric = [c for c in numeric_features if c not in special_numerics]\n",
    "\n",
    "\n",
    "# 3.4 General Numeric Cleaning\n",
    "# This used to crash because 'other_numeric' had ghost columns. \n",
    "# Now it is safe.\n",
    "if other_numeric:\n",
    "    print(f\"Cleaning general numeric columns: {other_numeric}\")\n",
    "    df[other_numeric] = df[other_numeric].replace([98, 99, 998, 999], np.nan)\n",
    "\n",
    "# 3.5 Lock all to float\n",
    "df[numeric_features] = df[numeric_features].astype(\"float\")\n",
    "\n",
    "print(\"\\n Numeric cleaning finished successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nominal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cooking fuel unique: [ 8 97 11 10  9  2  5  3  4  7  1 96  6]\n"
     ]
    }
   ],
   "source": [
    "# Just a quick peek to ensure '99' didn't survive in a nominal column\n",
    "print(\"Cooking fuel unique:\", df[\"cooking_fuel\"].unique()) \n",
    "# Should see codes like 1, 2, 3... or NaN. NOT 99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning 97/99 from 10 nominal columns...\n",
      "\n",
      "Cooking fuel unique values (should have 96, but no 97):\n",
      "[ 8. nan 11. 10.  9.  2.  5.  3.  4.  7.  1. 96.  6.]\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the specific codes to clean\n",
    "# 97 = Not a dejure resident (Missing)\n",
    "# 99 = Missing (We might have missed some in Step 5.2)\n",
    "nominal_missing_codes = [97, 99]\n",
    "\n",
    "# 2. Apply ONLY to nominal features\n",
    "# We keep 96 because \"Other\" is a valid category!\n",
    "if nominal_features:\n",
    "    print(f\"Cleaning 97/99 from {len(nominal_features)} nominal columns...\")\n",
    "    df[nominal_features] = df[nominal_features].replace(nominal_missing_codes, np.nan)\n",
    "\n",
    "# 3. Sanity Check for Cooking Fuel\n",
    "# Should see: 1, 2, 3... 96 (Other). NO 97.\n",
    "print(\"\\nCooking fuel unique values (should have 96, but no 97):\")\n",
    "print(df[\"cooking_fuel\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINAL SANITY CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===  FINAL DATASET SANITY CHECK ===\n",
      "\n",
      "Dataset Shape: (5705, 31)\n",
      "------------------------------\n",
      ">>> TARGET CHECK: 'anemia'\n",
      "anemia\n",
      "0    3230\n",
      "1    2475\n",
      "Name: count, dtype: int64\n",
      "------------------------------\n",
      ">>> BINARY FEATURES (7 cols)\n",
      "Binary check complete. (If no flags above, all are clean 0/1).\n",
      "------------------------------\n",
      ">>> ORDINAL FEATURES (5 cols)\n",
      "education_level: [np.float64(0.0), np.float64(1.0), np.float64(2.0), np.float64(3.0)]\n",
      "wealth_index: [np.float64(0.0), np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0)]\n",
      "freq_newspaper: [np.float64(0.0), np.float64(1.0), np.float64(2.0)]\n",
      "freq_radio: [np.float64(0.0), np.float64(1.0), np.float64(2.0)]\n",
      "freq_tv: [np.float64(0.0), np.float64(1.0), np.float64(2.0)]\n",
      "------------------------------\n",
      ">>> NOMINAL FEATURES (10 cols)\n",
      "marital_status: Clean. (Example val: 1)\n",
      "residence: Clean. (Example val: 2)\n",
      "region: Clean. (Example val: 1)\n",
      "floor_material: Clean. (Example val: 11.0)\n",
      "roof_material: Clean. (Example val: 31.0)\n",
      "wall_material: Clean. (Example val: 31.0)\n",
      "cooking_fuel: Clean. (Example val: 8.0)\n",
      "water_source: Clean. (Example val: 21.0)\n",
      "toilet_type: Clean. (Example val: 22.0)\n",
      "contraceptive_method: Clean. (Example val: 3)\n",
      "------------------------------\n",
      ">>> NUMERIC FEATURES (7 cols)\n",
      "                      min     max   count\n",
      "age                 13.00   49.00  5705.0\n",
      "household_size       1.00   31.00  5705.0\n",
      "age_first_marriage  10.00   39.00  5705.0\n",
      "living_children      0.00   10.00  5705.0\n",
      "time_to_water        0.00  997.00  5699.0\n",
      "altitude           -11.00   84.00  5705.0\n",
      "bmi                 13.07   46.47  5685.0\n",
      "------------------------------\n",
      "Ghost column check complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"===  FINAL DATASET SANITY CHECK ===\\n\")\n",
    "\n",
    "# 1. Check Dimensions\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 2. Check TARGET (Must be clean 0/1)\n",
    "print(\">>> TARGET CHECK: 'anemia'\")\n",
    "if \"anemia\" in df.columns:\n",
    "    print(df[\"anemia\"].value_counts(dropna=False).sort_index())\n",
    "else:\n",
    "    print(\" ERROR: Target 'anemia' is missing!\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 3. Check BINARY Features\n",
    "# Success Rule: Only [0.0, 1.0, nan]. No 7, 9, or other numbers.\n",
    "print(f\">>> BINARY FEATURES ({len(binary_features)} cols)\")\n",
    "# Filter list to what actually exists\n",
    "safe_binary = [c for c in binary_features if c in df.columns]\n",
    "for col in safe_binary:\n",
    "    uniques = sorted(df[col].dropna().unique())\n",
    "    # Alert if anything other than 0 or 1 exists\n",
    "    if not set(uniques).issubset({0.0, 1.0}):\n",
    "        print(f\" FLAG: {col} has unexpected values: {uniques}\")\n",
    "    else:\n",
    "        # Optional: Print only if you want to see everything\n",
    "        # print(f\"OK: {col}\")\n",
    "        pass\n",
    "print(\"Binary check complete. (If no flags above, all are clean 0/1).\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 4. Check ORDINAL Features\n",
    "# Success Rule: 0.0 to 4.0 range + nan. No 9, 99.\n",
    "print(f\">>> ORDINAL FEATURES ({len(ordinal_features)} cols)\")\n",
    "safe_ordinal = [c for c in ordinal_features if c in df.columns]\n",
    "for col in safe_ordinal:\n",
    "    print(f\"{col}: {sorted(df[col].dropna().unique())}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 5. Check NOMINAL Features\n",
    "# Success Rule: No 97, 98, 99. (96 \"Other\" is OK).\n",
    "print(f\">>> NOMINAL FEATURES ({len(nominal_features)} cols)\")\n",
    "safe_nominal = [c for c in nominal_features if c in df.columns]\n",
    "for col in safe_nominal:\n",
    "    # Check for forbidden codes\n",
    "    bad_codes = [97, 98, 99]\n",
    "    values = df[col].unique()\n",
    "    found_bad = [x for x in values if x in bad_codes]\n",
    "    if found_bad:\n",
    "        print(f\" ERROR: {col} still has codes: {found_bad}\")\n",
    "    else:\n",
    "        # Just print first 5 categories to verify it looks right\n",
    "        print(f\"{col}: Clean. (Example val: {values[0]})\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 6. Check NUMERIC Features\n",
    "# Success Rule: Reasonable min/max. No 9999.\n",
    "print(f\">>> NUMERIC FEATURES ({len(numeric_features)} cols)\")\n",
    "safe_numeric = [c for c in numeric_features if c in df.columns]\n",
    "# We use describe() to see ranges quickly\n",
    "print(df[safe_numeric].describe().T[[\"min\", \"max\", \"count\"]])\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 7. Check Ghost Columns\n",
    "# Ensure no dropped columns are lingering in your lists\n",
    "all_lists = safe_binary + safe_ordinal + safe_nominal + safe_numeric\n",
    "if \"years_schooling\" in all_lists:\n",
    "    print(\" ERROR: 'years_schooling' is still in a feature list!\")\n",
    "if \"hb\" in all_lists:\n",
    "    print(\" ERROR: 'hb' (target source) is in a feature list!\")\n",
    "print(\"Ghost column check complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/processed/processed_7.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling Missing values \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fill the NaN values we created. using the standard research approach:\n",
    "Numeric Features: Fill with Median (Robust to outliers).\n",
    "Categorical (Binary/Nominal/Ordinal): Fill with Mode (Most frequent value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing 7 numeric features...\n",
      "Imputing 22 categorical features...\n",
      "------------------------------\n",
      " SUCCESS: Dataset contains 0 missing values.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 1. Define Imputers\n",
    "# Numeric -> Median (Safe for skewed data like income/BMI)\n",
    "num_imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# Categorical -> Mode (Most Frequent)\n",
    "# We treat Binary and Ordinal as categorical for imputation purposes because\n",
    "# we want integers (e.g., 1.0 or 0.0), not decimals (0.45).\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "# 2. Impute Numeric Features\n",
    "# Filter list to what actually exists in df to avoid crashes\n",
    "numeric_features = [c for c in numeric_features if c in df.columns]\n",
    "print(f\"Imputing {len(numeric_features)} numeric features...\")\n",
    "df[numeric_features] = num_imputer.fit_transform(df[numeric_features])\n",
    "\n",
    "# 3. Impute Categorical Features (Nominal + Ordinal + Binary)\n",
    "# We combine them for this step to save code lines\n",
    "categorical_group = nominal_features + ordinal_features + binary_features\n",
    "# Filter to ensure we only touch columns that exist\n",
    "categorical_group = [c for c in categorical_group if c in df.columns]\n",
    "\n",
    "print(f\"Imputing {len(categorical_group)} categorical features...\")\n",
    "df[categorical_group] = cat_imputer.fit_transform(df[categorical_group])\n",
    "\n",
    "# 4. Final Missingness Check\n",
    "total_missing = df.isnull().sum().sum()\n",
    "print(\"-\" * 30)\n",
    "if total_missing == 0:\n",
    "    print(\" SUCCESS: Dataset contains 0 missing values.\")\n",
    "else:\n",
    "    print(f\" ERROR: Still found {total_missing} missing values!\")\n",
    "    # Optional: Print which columns are failing\n",
    "    print(df.columns[df.isnull().any()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET PREPROCESSING COMPLETE  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/processed/processed_8.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"is_pregnant\"].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change target for pregnant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"anemia\"] = np.where(\n",
    "    (df[\"is_pregnant\"] == 1) & (df[\"hb\"] < 11.0), 1,\n",
    "    np.where(\n",
    "        (df[\"is_pregnant\"] == 0) & (df[\"hb\"] < 12.0), 1,\n",
    "        0\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"anemia\"])\n",
    "df[\"anemia\"] = df[\"anemia\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checkpoint 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/processed/processed_9.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
